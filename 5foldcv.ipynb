{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is just to cross check the accuracy of the model with proper 5 fold cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from sklearn.model_selection import KFold\n",
    "import random \n",
    "import torch \n",
    "from torch import nn \n",
    "from torch import optim \n",
    "from torch.utils.data import DataLoader,TensorDataset, random_split,SubsetRandomSampler,ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms,datasets,models\n",
    "from torchsummary import summary\n",
    "from tqdm.notebook import tqdm \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the whole dataset into a vatiable, `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['set 1', 'set 2', 'set 3', 'set 4', 'set 5']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets=[]\n",
    "paths = [f'dataset/set {str(i+1)}' for i in range(5)]\n",
    "for path in paths:\n",
    "    sets.append(datasets.ImageFolder(path,transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.3080, 0.1464, 0.0501],[0.3346, 0.1610, 0.0558])\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sets = ConcatDataset(sets[:4])\n",
    "trainingSet, validationSet = random_split(training_sets,[864,96])\n",
    "\n",
    "train_loader = DataLoader(dataset=trainingSet,shuffle=True,num_workers=4,batch_size=4)\n",
    "validation_loader = DataLoader(dataset=validationSet,shuffle=False,num_workers=4,batch_size=4)\n",
    "\n",
    "testSet = DataLoader(dataset=sets[-1],shuffle=False,batch_size=4,num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Neural Neural Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b0(pretrained=True)\n",
    "model.classifier = nn.Sequential(nn.Dropout(0.5,inplace=True),\n",
    "                                    nn.Linear(1280,4,bias=True))\n",
    "model.to(device)\n",
    "summary(model,(3,512,512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write loss function, optimizer,  epochs and all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 40\n",
    "batch_size = 16\n",
    "k=5\n",
    "splits =KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "Details = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,trainloader,optimizer,lossFn,device):\n",
    "    '''\n",
    "    Accuracy := (TotalNumber_of_correct_predictions)/len(trainset)\n",
    "    \n",
    "    len(trainset) = len(trainloader)* Batch_size\n",
    "                OR\n",
    "    len(trainset) = len(trainloader)* len(images)\n",
    "                OR\n",
    "    len(trainset) = len(trainloader)* len(labels)\n",
    "\n",
    "    '''\n",
    "    trainLoss = 0.0\n",
    "    trainCorrect = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "    \n",
    "    for images, labels in tqdm(trainloader):\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(images) # input format [B.C,W,H]\n",
    "        loss = lossFn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += labels.size(0) # this will add the length of the labels\n",
    "        # Bacically collecting the batch size. At the end of the epoch this\n",
    "        # will be batch_size*len(trainloader) | labels.size(0) == batch_size\n",
    "        \n",
    "        trainLoss += loss.item() \n",
    "        preds = output.argmax(dim=1)\n",
    "        trainCorrect += (preds==labels).cpu().detach().sum().item()        \n",
    "        # trainCorrect += (preds.cpu().detach()==labels.cpu().detach()).sum().item()\n",
    "    print('total',total,'trainDl len',len(train_loader))\n",
    "    return trainLoss,(trainCorrect/total)*100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Valid(dataloader,model,loss_fn):\n",
    "    correct =0\n",
    "    total=0\n",
    "    loss =0\n",
    "    model.eval()\n",
    "\n",
    "    for images,labels in tqdm(dataloader):\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "\n",
    "        output = model(images,labels)\n",
    "        predictions = output.argmax(1)\n",
    "        lossPerBatch = loss_fn(output,labels)\n",
    "        total+=labels.size(0)\n",
    "        loss +=lossPerBatch\n",
    "        correct+= (predictions==labels).cpu().detach().sum().item()\n",
    "\n",
    "    return loss,(correct/total)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel(model,trainDl,valDl, device,lossFn,optimizer,epochs):\n",
    "    trainLoss,valLoss,trainAcc,valAcc = [],[],[],[]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch+1)\n",
    "        tl,ta = training(model,trainloader=trainDl,optimizer=optimizer,lossFn=lossFn,device=device)\n",
    "        trainLoss.append(tl)\n",
    "        trainAcc.append(ta)\n",
    "        vl,va = Valid(valDl,model,loss_fn=lossFn)\n",
    "        valLoss.append(vl)\n",
    "        valAcc.append(va)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c74d99681ad1a2c591eba718ee02a1ecc6e61532130e8f76172a42e8769d33f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
